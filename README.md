# Disinformation Detection Pipeline â€“ TNS Final Project

### Group Information
**Group Number:** 
 * Group 2
**Group Members:**
* Philip Tham
* Bhoomika Mehta
* Ian Zheng

---

### Submitted Files & Description

Below is a list of all source code, configuration, and data files included in this submission:

**Core Logic & Scripts**
* **`policy_proposal_labeler.py`**: The main class file containing the `DisinformationLabeler`. It handles the logic for loading models (SentenceTransformer), performing fuzzy matching on sources, calculating semantic similarity, and computing the final weighted probability score.
* **`run_evaluation.py`**: The primary executable for testing. It loads the unseen test data, runs the labeler against it, calculates performance metrics (Accuracy, Precision, Recall), and saves the confusion matrix data.
* **`generate_resources.py`**: A utility script used to split the raw dataset into tuning/testing sets (stratified) and compile the "Knowledge Base" of known disinformation narratives from various CSV sources.
* **`tune_thresholds.py`**: An optimization script that iterates through the tuning dataset to calculate the optimal decision threshold (maximizing the F0.5 score) and updates `config.json` automatically.
* **`graph.py`**: A visualization script that reads the numpy confusion matrix generated by the evaluation script and plots a heatmap using Seaborn/Matplotlib.
* **`config.json`**: Stores the tunable parameters (weights for source vs. content, and the decision threshold) to ensure consistent behavior across scripts.

**Data Directory (`data/`)**
* **`known_narratives.csv`**: The generated Knowledge Base containing specific disinformation narratives used for semantic matching.
* **`sus_tele.csv`**: A blacklist of suspicious Telegram handles used for source verification.
* **`disinfo_titles_cleaned.csv`** & **`more_disinfo.csv`**: Auxiliary datasets containing disinformation headlines used to build the Knowledge Base.
* **`tuning_data.csv`** & **`test_data.csv`**: The split datasets generated by `generate_resources.py` (included to ensure reproducibility of our specific metrics).

**Graphs Directory (`graphs/`)**
* **`confusion_matrix_data.npy`**: The binary numpy file storing the results of the evaluation for plotting.

---

### How to Run the Test

Follow the steps below to reproduce our results and generate the performance metrics.

**1. Install Dependencies**
Ensure you have the required Python libraries installed:
```bash
pip install pandas numpy scikit-learn rapidfuzz sentence-transformers matplotlib seaborn
```

**2. Generate Data Splits & Knowledge Base (Optional)**
*Note: The submission already includes the processed `test_data.csv` and `known_narratives.csv`. If you wish to regenerate them from scratch:*

```bash
python generate_resources.py
```

**3. Run the Evaluation (The Main Test)**
This script runs the labeler against the held-out test set and outputs the final Accuracy, Precision, and Recall scores to the console.

```bash
python run_evaluation.py
```

*Expected Output:*

  * Console printout of metrics.
  * Generation of `graphs/confusion_matrix_data.npy`.

**4. Generate Visualization**
To create the confusion matrix image based on the test results:

```bash
python graph.py
```

*Output:* An image file `graphs/confusion_matrix.png` will be created.
